# Data Engineering for Streaming Data Notes
The following notes are from the second section of the Google Cloud Big Data and Machine Learning Fundamentals Course.

## Introduction
* Ingestion of streaming data with Pub/Sub.
* Process data with Dataflow.
* Visualize with Data Studio / Looker.
* Normally in between Dataflow and visualization the data is saved & analyzed in a data warehouse, like BigQuery.

* Batch processing is when processing and analysis happens on a set of stored data, e.g. payroll that is processed every week.

* Streaming data is a flow of data generated from various sources.
* Processing happens as it flows through the system.
* Analysis & reporting of events as it happens.
* Example: fraud detection.

## Big data challenges
* Scalable and reliable pipelines are a data engineer's responsibility.
* Both data engineers and data scientists face four major challenges, known as the 4Vs.
    * Variety
    * Volume
    * Velocity 
    * Veracity

### Variety
* Data could come in from a different sources in different formats. E.g. Self-driving cars, sensors, audio, visualize.

### Volume
* Can your pipeline code and infrastructure scale with petabytes of data coming in?

### Velocity
* Data needs to be processed in near real-time.
* It may arrive late, has bad data and needs to be processed.

### Veracity
* Data quality.
* Possibility gathered data may have inconsistencies & uncertainties.

## Message-oriented architecture
* Early stage is data ingestion.
* Streaming data is received.
